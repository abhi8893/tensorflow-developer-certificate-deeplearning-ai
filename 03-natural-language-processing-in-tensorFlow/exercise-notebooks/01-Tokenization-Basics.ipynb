{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01-Tokenization-Basics.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1ebbwjKUPDzACwGdQXRtzq7T0zOmeRwI9","authorship_tag":"ABX9TyN22A0touCkhFckT2HS93gK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"ocGiLEQbFasY"},"source":["# Tokenization Basics"]},{"cell_type":"code","metadata":{"id":"ky228MgcHbMw","executionInfo":{"status":"ok","timestamp":1627106732439,"user_tz":-330,"elapsed":390,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}}},"source":["%load_ext autoreload\n","%autoreload 2"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AuJw9hoMHJeQ"},"source":["## Install `tfutils`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44yqMPXHHNkd","executionInfo":{"status":"ok","timestamp":1627106710860,"user_tz":-330,"elapsed":10858,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}},"outputId":"63a7ed7d-a7a9-4a21-e5b4-0535497fc7f6"},"source":["!pip install -e /content/drive/MyDrive/projects/tfutils"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Obtaining file:///content/drive/MyDrive/projects/tfutils\n","Installing collected packages: tfutils\n","  Running setup.py develop for tfutils\n","Successfully installed tfutils-0.0.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cxQmm2P2IC2R"},"source":["## Basic Tokenization\n","\n","\n","* Think of a good tokenization rule (preferably custom)\n","* Use stopwords appropriately (make a custom list)\n","* Spell check!\n","* Handle contractions like `haven't` -> `have not`\n","* After tokenization, check \n","    * number of OOV tokens on test set (see if a rule augment and convert these to in vocab tokens)\n","    * which tokens were stripped off in train set (see if some tokens are what we would want to keep)\n","    * Visualize the frequencies of words, most common, least common\n"]},{"cell_type":"code","metadata":{"id":"4IdAUdc3LDzL","executionInfo":{"status":"ok","timestamp":1627110253631,"user_tz":-330,"elapsed":709,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}}},"source":["def sequences_to_text_tokens(sequences, w2id, zero_token='<PAD>'):\n","\n","    text_tokens = [None]*len(sequences)\n","    id2w = {v:k for k, v in w2id.items()}\n","\n","    for i, seq in enumerate(sequences):\n","        seq_len = len(seq)\n","        text_tokens_lst = [None]*seq_len\n","        for j in range(seq_len):\n","            token_id = seq[j]\n","            if token_id != 0:\n","                text_tokens_lst[j] = id2w.get(token_id)\n","            else:\n","                text_tokens_lst[j] = zero_token\n","\n","        text_tokens[i] = text_tokens_lst\n","\n","    \n","    return text_tokens\n"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"id":"7B3wybZrHef_","executionInfo":{"status":"ok","timestamp":1627110406477,"user_tz":-330,"elapsed":395,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}}},"source":["from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Train sentences\n","sentences = ['I love tensorflow',\n","             'i love tensorflow!!',\n","             'you love TenSorflow',\n","             'we all LOVE Tensorflow :)']\n","\n","# Initialize a tokenizer with apt tokenization rules\n","tokenizer = Tokenizer(num_words=100, oov_token='<OOV>')\n","tokenizer.fit_on_texts(sentences)\n","\n","# Get word to index, index to word dicts\n","w2id = tokenizer.word_index\n","id2w = {v:k for k, v in w2id.items()}\n","\n","# Convert train sentences to sequences\n","sents_token_ids = tokenizer.texts_to_sequences(sentences)\n","sents_tokens = sequences_to_text_tokens(sents_token_ids, w2id)\n","\n","# maximum sequence length\n","max_seq_len = max([len(sent_token_ids) for sent_token_ids in sents_token_ids])\n","\n","# padded sequences\n","sents_token_ids_padded = pad_sequences(sents_token_ids, maxlen=max_seq_len, padding='post', truncating='post')\n"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uepo9QquIeIr","executionInfo":{"status":"ok","timestamp":1627110419053,"user_tz":-330,"elapsed":371,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}},"outputId":"09b108e7-93eb-4b53-fbc0-e06fd562cbd9"},"source":["print('Word to Index: ', w2id)\n","print('\\n', '-'*10, '\\n')\n","print('Index to Word: ', id2w)"],"execution_count":63,"outputs":[{"output_type":"stream","text":["Word to Index:  {'<OOV>': 1, 'love': 2, 'tensorflow': 3, 'i': 4, 'you': 5, 'we': 6, 'all': 7}\n","\n"," ---------- \n","\n","Index to Word:  {1: '<OOV>', 2: 'love', 3: 'tensorflow', 4: 'i', 5: 'you', 6: 'we', 7: 'all'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hti5s7k_JLZf","executionInfo":{"status":"ok","timestamp":1627110623210,"user_tz":-330,"elapsed":390,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}},"outputId":"01e6759c-12d6-411f-c222-374f1cd3fc52"},"source":["for sent, sent_tokens, sent_token_ids, sent_token_ids_padded in zip(sentences, sents_tokens, sents_token_ids, sents_token_ids_padded):\n","    print('original: ', sent)\n","    print('tokenized text: ', sent_tokens)\n","    print('tokenized ids: ', sent_token_ids)\n","    print('tokenized ids (trunc+pad): ', sent_token_ids_padded)\n","    print('-'*10)"],"execution_count":74,"outputs":[{"output_type":"stream","text":["original:  I love tensorflow\n","tokenized text:  ['<OOV>', 'i', '<OOV>', 'love', 'tensorflow']\n","tokenized ids:  [1, 4, 1, 2, 3]\n","tokenized ids (trunc+pad):  [1 4 1 2]\n","----------\n","original:  i love tensorflow!!\n","tokenized text:  ['i', '<OOV>', 'tensorflow']\n","tokenized ids:  [4, 1, 3]\n","tokenized ids (trunc+pad):  [4 1 3 0]\n","----------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4wFn74bIKe3","executionInfo":{"status":"ok","timestamp":1627110570469,"user_tz":-330,"elapsed":420,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}}},"source":["new_sents  = ['Hello, I too love tensorflow!', 'i like tensorflow']\n","sents_token_ids = tokenizer.texts_to_sequences(new_sents)\n","sents_tokens = sequences_to_text_tokens(sents_token_ids, w2id)\n","sents_decoded = tokenizer.sequences_to_texts(sents_token_ids)\n","sents_token_ids_padded = pad_sequences(sents_token_ids, maxlen=max_seq_len, padding='post', truncating='post')"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PUUVVFsPW18V","executionInfo":{"status":"ok","timestamp":1627110742402,"user_tz":-330,"elapsed":420,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}},"outputId":"f900bc32-9f55-437e-d0a0-908b3388ab0a"},"source":["tokenizer.sequences_to_texts(sents_token_ids_padded)"],"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['<OOV> i <OOV> love', 'i <OOV> tensorflow <OOV>']"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q8WR0IB1JSUU","executionInfo":{"status":"ok","timestamp":1627110799527,"user_tz":-330,"elapsed":394,"user":{"displayName":"Abhishek Bhatia","photoUrl":"","userId":"05636063588651340588"}},"outputId":"ce7c1360-68c9-41d8-fa8f-fdaec842f6a4"},"source":["for sent, sent_tokens, sent_token_ids, sent_token_ids_padded, sent_decoded in zip(new_sents, sents_tokens, \n","                                                                    sents_token_ids, sents_token_ids_padded,\n","                                                                    sents_decoded):\n","    print('original: ', sent)\n","    print('tokenized text: ', sent_tokens)\n","    print('tokenized ids: ', sent_token_ids)\n","    print('tokenized ids (trunc+pad): ', sent_token_ids_padded)\n","    print('decoded: ', sent_decoded)\n","    print('-'*10)"],"execution_count":77,"outputs":[{"output_type":"stream","text":["original:  Hello, I too love tensorflow!\n","tokenized text:  ['<OOV>', 'i', '<OOV>', 'love', 'tensorflow']\n","tokenized ids:  [1, 4, 1, 2, 3]\n","tokenized ids (trunc+pad):  [1 4 1 2]\n","decoded:  <OOV> i <OOV> love tensorflow\n","----------\n","original:  i like tensorflow\n","tokenized text:  ['i', '<OOV>', 'tensorflow']\n","tokenized ids:  [4, 1, 3]\n","tokenized ids (trunc+pad):  [4 1 3 0]\n","decoded:  i <OOV> tensorflow\n","----------\n"],"name":"stdout"}]}]}